{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lucas\\Documents\\GitHub\\OrangeDetect\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Egyptian cat\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "from torcheval.metrics import MulticlassF1Score, MulticlassPrecision, MulticlassRecall, MulticlassConfusionMatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier.out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our classification layer\n",
    "\n",
    "# freeze the layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replaces the FC layer for our classes\n",
    "num_ftrs = model.classifier.in_features\n",
    "model.classifier = nn.Linear(num_ftrs, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the model has 3 output features instead of 1000 in the classifier layer\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our functions\n",
    "\n",
    "def dataload(path):\n",
    "\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()]) # I removed the normalize part because of the tokenizer from the ViT\n",
    "\n",
    "    path_test = os.path.join(path, \"test\")\n",
    "    path_train = os.path.join(path, \"train\")\n",
    "    path_eval = os.path.join(path, \"eval\")\n",
    "\n",
    "    test = ImageFolder(root=path_test, transform=transform) # Automatically classifies the folder order as 0, 1 and 2 respectively\n",
    "    train = ImageFolder(root=path_train, transform=transform)\n",
    "    eval = ImageFolder(root=path_eval, transform=transform)\n",
    "\n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(train, batch_size=32, num_workers=4,shuffle=True)\n",
    "    test_dataloader = DataLoader(test, batch_size=32, num_workers=4,shuffle=True)\n",
    "    eval_dataloader = DataLoader(eval, batch_size=32, num_workers=4,shuffle=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader, eval_dataloader\n",
    "\n",
    "train_dataloader, test_dataloader, eval_dataloader = dataload(\"C:/Users/Lucas/Documents/GitHub/OrangeDetect/data/processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs =1 \n",
    "device ='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Configuração do otimizador\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=3e-4,  # learning rate padrão usado no paper\n",
    "    weight_decay=0.3,  # weight decay para regularização\n",
    "    betas=(0.9, 0.999)  # valores padrão do Adam\n",
    ")\n",
    "\n",
    "# Learning rate scheduler (opcional, mas recomendado)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=epochs,  # número total de épocas\n",
    "    eta_min=1e-6   # learning rate mínimo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, (X, y) in enumerate(train_dataloader):\n",
    "    test = processor(X, return_tensors=\"pt\", do_rescale=False)\n",
    "    model(**test)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------ Epoch 1 ------\n",
      "Loss: 1.2162\n",
      "Loss: 0.1423\n",
      "Loss: 0.0762\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\n------ Epoch {epoch + 1} ------')\n",
    "    \n",
    "    # Training phase\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        X = processor(X, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(**X)\n",
    "        loss = criterion(pred['logits'], y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print(f'Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n",
      "Accuracy: 96.00%\n",
      "Mean Loss: 0.1826\n",
      "F1 Score: 0.9600\n",
      "Precision: 0.9600\n",
      "Recall: 0.9600\n"
     ]
    }
   ],
   "source": [
    "print('\\nEvaluating...')\n",
    "\n",
    "# Evaluation phase\n",
    "model.eval()\n",
    "correct = 0\n",
    "total_loss = 0\n",
    "y_correct = []\n",
    "y_pred = []\n",
    "\n",
    "# Initialize metrics\n",
    "f1score = MulticlassF1Score(num_classes=3)\n",
    "precision_metric = MulticlassPrecision(num_classes=3)\n",
    "recall_metric = MulticlassRecall(num_classes=3)\n",
    "\n",
    "metrics = {\n",
    "    'loss': [],\n",
    "    'correct': [],\n",
    "    'f1': [],\n",
    "    'precision': [],\n",
    "    'recall': []\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_test, y_test in eval_dataloader:\n",
    "        X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "        pred = model(X_test)\n",
    "        #pred = pred['logits'],\n",
    "        loss = criterion(pred['logits'], y_test).item()\n",
    "        total_loss += loss\n",
    "\n",
    "        predicted = pred['logits'].argmax(dim=1)\n",
    "        correct_batch = (predicted == y_test).sum().item()\n",
    "        correct += correct_batch\n",
    "\n",
    "        y_pred.append(predicted)\n",
    "        y_correct.append(y_test)\n",
    "\n",
    "        # Update metrics\n",
    "        f1score.update(y_test, predicted)\n",
    "        precision_metric.update(y_test, predicted)\n",
    "        recall_metric.update(y_test, predicted)\n",
    "\n",
    "        metrics['loss'].append(loss)\n",
    "        metrics['correct'].append(correct_batch)\n",
    "\n",
    "# Compute final metrics\n",
    "metrics['f1'] = f1score.compute().tolist()\n",
    "metrics['precision'] = precision_metric.compute().tolist()\n",
    "metrics['recall'] = recall_metric.compute().tolist()\n",
    "\n",
    "accuracy = correct / len(eval_dataloader.dataset)\n",
    "mean_loss = sum(metrics['loss']) / len(metrics['loss'])\n",
    "\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'Mean Loss: {mean_loss:.4f}')\n",
    "print(f'F1 Score: {metrics[\"f1\"]:.4f}')\n",
    "print(f'Precision: {metrics[\"precision\"]:.4f}')\n",
    "print(f'Recall: {metrics[\"recall\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.15992891788482666,\n",
       "  0.2399546504020691,\n",
       "  0.21071556210517883,\n",
       "  0.10720616579055786,\n",
       "  0.17166641354560852,\n",
       "  0.3222771883010864,\n",
       "  0.15104974806308746,\n",
       "  0.16683505475521088,\n",
       "  0.13099372386932373,\n",
       "  0.16512082517147064],\n",
       " 'correct': [32, 31, 30, 32, 30, 27, 31, 31, 32, 12],\n",
       " 'f1': 0.9599999785423279,\n",
       " 'precision': 0.9599999785423279,\n",
       " 'recall': 0.9599999785423279}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
