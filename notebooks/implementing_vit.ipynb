{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\lucas\\documents\\github\\orangedetect\\venv\\lib\\site-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\lucas\\documents\\github\\orangedetect\\venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\lucas\\documents\\github\\orangedetect\\venv\\lib\\site-packages (from transformers) (0.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lucas\\documents\\github\\orangedetect\\venv\\lib\\site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lucas\\documents\\github\\orangedetect\\venv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lucas\\documents\\github\\orangedetect\\venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lucas\\documents\\github\\orangedetect\\venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\lucas\\documents\\github\\orangedetect\\venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\lucas\\documents\\github\\orangedetect\\venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\lucas\\documents\\github\\orangedetect\\venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lucas\\documents\\github\\orangedetect\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lucas\\documents\\github\\orangedetect\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lucas\\documents\\github\\orangedetect\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucas\\documents\\github\\orangedetect\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lucas\\documents\\github\\orangedetect\\venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lucas\\documents\\github\\orangedetect\\venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lucas\\documents\\github\\orangedetect\\venv\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lucas\\documents\\github\\orangedetect\\venv\\lib\\site-packages (from requests->transformers) (2025.4.26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lucas\\Documents\\GitHub\\OrangeDetect\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Egyptian cat\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "from torcheval.metrics import MulticlassF1Score, MulticlassPrecision, MulticlassRecall, MulticlassConfusionMatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier.out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our classification layer\n",
    "\n",
    "# freeze the layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replaces the FC layer for our classes\n",
    "num_ftrs = model.classifier.in_features\n",
    "model.classifier = nn.Linear(num_ftrs, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the model has 3 output features instead of 1000 in the classifier layer\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our functions\n",
    "\n",
    "def dataload(path):\n",
    "\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()]) # I removed the normalize part because of the tokenizer from the ViT\n",
    "\n",
    "    path_test = os.path.join(path, \"test\")\n",
    "    path_train = os.path.join(path, \"train\")\n",
    "    path_eval = os.path.join(path, \"eval\")\n",
    "\n",
    "    test = ImageFolder(root=path_test, transform=transform) # Automatically classifies the folder order as 0, 1 and 2 respectively\n",
    "    train = ImageFolder(root=path_train, transform=transform)\n",
    "    eval = ImageFolder(root=path_eval, transform=transform)\n",
    "\n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(train, batch_size=32, num_workers=4,shuffle=True)\n",
    "    test_dataloader = DataLoader(test, batch_size=32, num_workers=4,shuffle=True)\n",
    "    eval_dataloader = DataLoader(eval, batch_size=32, num_workers=4,shuffle=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader, eval_dataloader\n",
    "\n",
    "train_dataloader, test_dataloader, eval_dataloader = dataload(\"C:/Users/Lucas/Documents/GitHub/OrangeDetect/data/processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs =1 \n",
    "device ='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=3e-4,  \n",
    "    weight_decay=0.3,  \n",
    "    betas=(0.9, 0.999)  \n",
    ")\n",
    "\n",
    "# Learning rate scheduler \n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=epochs,  \n",
    "    eta_min=1e-6   #\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, (X, y) in enumerate(train_dataloader):\n",
    "    test = processor(X, return_tensors=\"pt\", do_rescale=False)\n",
    "    model(**test)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------ Epoch 1 ------\n",
      "Loss: 1.3606\n",
      "Loss: 0.1581\n",
      "Loss: 0.1262\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\n------ Epoch {epoch + 1} ------')\n",
    "    \n",
    "    # Training phase\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        X = processor(X, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(**X)\n",
    "        loss = criterion(pred['logits'], y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print(f'Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n",
      "Accuracy: 96.67%\n",
      "Mean Loss: 0.1657\n",
      "F1 Score: 0.9667\n",
      "Precision: 0.9667\n",
      "Recall: 0.9667\n"
     ]
    }
   ],
   "source": [
    "print('\\nEvaluating...')\n",
    "\n",
    "# Evaluation phase\n",
    "model.eval()\n",
    "correct = 0\n",
    "total_loss = 0\n",
    "y_correct = []\n",
    "y_pred = []\n",
    "\n",
    "# Initialize metrics\n",
    "f1score = MulticlassF1Score(num_classes=3)\n",
    "precision_metric = MulticlassPrecision(num_classes=3)\n",
    "recall_metric = MulticlassRecall(num_classes=3)\n",
    "\n",
    "metrics = {\n",
    "    'loss': [],\n",
    "    'correct': [],\n",
    "    'f1': [],\n",
    "    'precision': [],\n",
    "    'recall': []\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_test, y_test in eval_dataloader:\n",
    "        X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "        pred = model(X_test)\n",
    "        #pred = pred['logits'],\n",
    "        loss = criterion(pred['logits'], y_test).item()\n",
    "        total_loss += loss\n",
    "\n",
    "        predicted = pred['logits'].argmax(dim=1)\n",
    "        correct_batch = (predicted == y_test).sum().item()\n",
    "        correct += correct_batch\n",
    "\n",
    "        y_pred.append(predicted)\n",
    "        y_correct.append(y_test)\n",
    "\n",
    "        # Update metrics\n",
    "        f1score.update(y_test, predicted)\n",
    "        precision_metric.update(y_test, predicted)\n",
    "        recall_metric.update(y_test, predicted)\n",
    "\n",
    "        metrics['loss'].append(loss)\n",
    "        metrics['correct'].append(correct_batch)\n",
    "\n",
    "# Compute final metrics\n",
    "metrics['f1'] = f1score.compute().tolist()\n",
    "metrics['precision'] = precision_metric.compute().tolist()\n",
    "metrics['recall'] = recall_metric.compute().tolist()\n",
    "\n",
    "accuracy = correct / len(eval_dataloader.dataset)\n",
    "mean_loss = sum(metrics['loss']) / len(metrics['loss'])\n",
    "\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'Mean Loss: {mean_loss:.4f}')\n",
    "print(f'F1 Score: {metrics[\"f1\"]:.4f}')\n",
    "print(f'Precision: {metrics[\"precision\"]:.4f}')\n",
    "print(f'Recall: {metrics[\"recall\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.18581648170948029,\n",
       "  0.19089628756046295,\n",
       "  0.13942116498947144,\n",
       "  0.124359130859375,\n",
       "  0.18860331177711487,\n",
       "  0.09645327925682068,\n",
       "  0.2514939606189728,\n",
       "  0.19607584178447723,\n",
       "  0.10955896973609924,\n",
       "  0.1739671677350998],\n",
       " 'correct': [30, 31, 31, 32, 30, 32, 29, 32, 31, 12],\n",
       " 'f1': 0.9666666388511658,\n",
       " 'precision': 0.9666666388511658,\n",
       " 'recall': 0.9666666388511658}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred['logits'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.3333, 0.3373, 0.4078,  ..., 0.4902, 0.2471, 0.0118],\n",
       "          [0.3569, 0.3373, 0.3412,  ..., 0.4980, 0.2471, 0.0118],\n",
       "          [0.3608, 0.3373, 0.3333,  ..., 0.5059, 0.2667, 0.0118],\n",
       "          ...,\n",
       "          [0.5333, 0.5451, 0.5843,  ..., 0.1725, 0.2510, 0.3412],\n",
       "          [0.5294, 0.5294, 0.5451,  ..., 0.3490, 0.3686, 0.4039],\n",
       "          [0.5490, 0.5451, 0.5294,  ..., 0.2824, 0.3020, 0.3490]],\n",
       "\n",
       "         [[0.4118, 0.4118, 0.4000,  ..., 0.5843, 0.2941, 0.0196],\n",
       "          [0.4314, 0.4118, 0.4000,  ..., 0.5843, 0.2980, 0.0235],\n",
       "          [0.4353, 0.4039, 0.4000,  ..., 0.5961, 0.3176, 0.0275],\n",
       "          ...,\n",
       "          [0.6431, 0.6588, 0.7020,  ..., 0.2000, 0.2980, 0.4039],\n",
       "          [0.6510, 0.6510, 0.6588,  ..., 0.4078, 0.4431, 0.4784],\n",
       "          [0.6706, 0.6667, 0.6471,  ..., 0.3412, 0.3647, 0.4039]],\n",
       "\n",
       "         [[0.2980, 0.3216, 0.3373,  ..., 0.3412, 0.1647, 0.0118],\n",
       "          [0.2980, 0.3020, 0.3098,  ..., 0.3412, 0.1647, 0.0118],\n",
       "          [0.2745, 0.2706, 0.2902,  ..., 0.3333, 0.1804, 0.0078],\n",
       "          ...,\n",
       "          [0.3451, 0.3569, 0.3961,  ..., 0.1373, 0.1961, 0.2471],\n",
       "          [0.3490, 0.3412, 0.3608,  ..., 0.2549, 0.2667, 0.2784],\n",
       "          [0.4000, 0.3804, 0.3490,  ..., 0.2118, 0.2196, 0.2314]]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),  # Convert to tensor first\n",
    "])\n",
    "\n",
    "img = cv2.imread(r'C:\\Users\\Lucas\\Documents\\GitHub\\OrangeDetect\\ck1.jpg')\n",
    "img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "img_tensor = transform(img).unsqueeze(0).to(device='cuda')\n",
    "result = model(img_tensor)['logits'].argmax()\n",
    "\n",
    "# Going to change this to Image.open\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
