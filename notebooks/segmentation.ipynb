{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lucas\\Documents\\GitHub\\OrangeDetect\\venv\\Lib\\site-packages\\torchvision\\utils.py:233: UserWarning: Argument 'font_size' will be ignored since 'font' is not set.\n",
      "  warnings.warn(\"Argument 'font_size' will be ignored since 'font' is not set.\")\n"
     ]
    }
   ],
   "source": [
    "from torchvision.io.image import decode_image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "img = decode_image(r\"C:\\Users\\Lucas\\Documents\\GitHub\\OrangeDetect\\2m9ewge9sd681.jpg\")\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = [preprocess(img)]\n",
    "\n",
    "# Step 4: Use the model and visualize the prediction\n",
    "prediction = model(batch)[0]\n",
    "labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "box = draw_bounding_boxes(img, boxes=prediction[\"boxes\"],\n",
    "                          labels=labels,\n",
    "                          colors=\"red\",\n",
    "                          width=4, font_size=30)\n",
    "im = to_pil_image(box.detach())\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([[ 925.4276, 1426.9391, 2195.3459, 2674.1357],\n",
       "         [1089.1602,   73.4330, 2286.5332, 1289.4617],\n",
       "         [2236.9993, 1299.5852, 2880.9663, 2275.0403],\n",
       "         [   0.0000, 1504.4923,  997.4054, 2585.3723],\n",
       "         [2029.5212,  223.2374, 2874.3406, 1336.7885],\n",
       "         [ 403.0679,  582.5072, 1597.8832, 1434.3579],\n",
       "         [ 315.8342, 1106.6494, 1402.6467, 1748.0195]],\n",
       "        grad_fn=<StackBackward0>),\n",
       " 'labels': tensor([55, 55, 55, 55, 55, 55, 55]),\n",
       " 'scores': tensor([0.9985, 0.9955, 0.9927, 0.9902, 0.9869, 0.9856, 0.9695],\n",
       "        grad_fn=<IndexBackward0>)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O intuito aqui é extrairmos as laranjas segmentadas pelas bounding boxes que o modelo preveu, assim mandando as laranjas para o modelo classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['orange', 'orange', 'orange', 'orange', 'orange', 'orange', 'orange']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Acessando as labels\n",
    "# O código acima em questão é da própria documentação do Pytorch. Isso vai ser a base para nosso código\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([[ 925.4276, 1426.9391, 2195.3459, 2674.1357],\n",
       "         [1089.1602,   73.4330, 2286.5332, 1289.4617],\n",
       "         [2236.9993, 1299.5852, 2880.9663, 2275.0403],\n",
       "         [   0.0000, 1504.4923,  997.4054, 2585.3723],\n",
       "         [2029.5212,  223.2374, 2874.3406, 1336.7885],\n",
       "         [ 403.0679,  582.5072, 1597.8832, 1434.3579],\n",
       "         [ 315.8342, 1106.6494, 1402.6467, 1748.0195]],\n",
       "        grad_fn=<StackBackward0>),\n",
       " 'labels': tensor([55, 55, 55, 55, 55, 55, 55]),\n",
       " 'scores': tensor([0.9985, 0.9955, 0.9927, 0.9902, 0.9869, 0.9856, 0.9695],\n",
       "        grad_fn=<IndexBackward0>)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Acessando o output\n",
    "# O que nos interessa mesmo é apenas as boxes. O resto é irrelevante\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_segment_torch(img, bbox):\n",
    "    \"\"\"\n",
    "    Recorta um segmento da imagem com base na bounding box fornecida.\n",
    "\n",
    "    img: Tensor da imagem (C, H, W) ou (H, W, C)\n",
    "    bbox: Tensor com [x1, y1, x2, y2]\n",
    "    \"\"\"\n",
    "    # Garantir que a bounding box é do tipo inteiro\n",
    "    bbox = bbox.long()\n",
    "    x1, y1, x2, y2 = bbox\n",
    "\n",
    "    # Se a imagem estiver no formato (C, H, W), converte pra (H, W, C)\n",
    "    if img.dim() == 3 and img.shape[0] <= 4:  # assume (C, H, W)\n",
    "        img = img.permute(1, 2, 0)\n",
    "\n",
    "    # Recorta o segmento\n",
    "    segmento = img[y1:y2, x1:x2]\n",
    "\n",
    "    return segmento\n",
    "\n",
    "    \n",
    "# Código gerado por IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for box in prediction['boxes']:\n",
    "    imgs = extract_segment_torch(img, box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = extract_segment_torch(img, prediction['boxes'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = imgs.permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = to_pil_image(imgs.detach())\n",
    "\n",
    "imgs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for box in prediction['boxes']:\n",
    "    imgs = extract_segment_torch(img, box)\n",
    "    imgs = imgs.permute(2, 0, 1)\n",
    "    imgs = to_pil_image(imgs.detach())\n",
    "    imgs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Adiciona a raiz do projeto no path\n",
    "sys.path.append(os.path.abspath('..'))  # sobe uma pasta\n",
    "\n",
    "from src import Detect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conseguimos extrair o objeto segmentando da imagem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14752\\72677878.py:9: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  imgs= np.array(imgs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# Vamos testar usando o classe Detect \n",
    "\n",
    "import os\n",
    "\n",
    "model_path = os.getenv('MODELS_FOLDER')\n",
    "\n",
    "for box in prediction['boxes']: \n",
    "    imgs = extract_segment_torch(img, box)\n",
    "    imgs= np.array(imgs)\n",
    "    orange = Detect(model_path, imgs)\n",
    "    print(orange.pred())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
